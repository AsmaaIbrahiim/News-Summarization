{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     self signed certificate in certificate chain\n",
      "[nltk_data]     (_ssl.c:1123)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preprocessing(text):\n",
    "    original_sentences = sent_tokenize(text.lower())\n",
    "    sentences = []\n",
    "    # Sentences Pre-processing\n",
    "    for sent in original_sentences:\n",
    "        tokens = [item for item in word_tokenize(sent) if item not in my_stopwords and item != '.' and item.isalpha()] # Preprocessing\n",
    "        sentences.append(' '.join([stemmer.stem(token) for token in tokens])) # Stemming\n",
    "    return original_sentences, sentences\n",
    "\n",
    "def word_tf(word,sentence):\n",
    "    #tf_score =  sentence.count(word)/ len(sentence)\n",
    "    return sentence.count(word)/ len(sentence)\n",
    "\n",
    "def word_idf(word, sentences):\n",
    "    return math.log10(' '.join(sentences).count(word)/ len(sentences))\n",
    "\n",
    "def word_in_sentence_tfidf(word, sentence, sentences):\n",
    "    return word_tf(word,sentence)*word_idf(word, sentences)\n",
    "\n",
    "def sentence_score(sentence, sentences):\n",
    "    return sum([word_in_sentence_tfidf(word, sentence, sentences) for word in sentence])\n",
    "\n",
    "def sentences_scores(sentences):\n",
    "    return [sentence_score(sent, sentences) for sent in sentences]\n",
    "\n",
    "def get_best_k_sentences_indecies(k, sentences, original_sentences):\n",
    "    #lista = sorted(list(np.argsort(sentences_scores(sentences))[-k:-1]))\n",
    "    return sorted(list(np.argsort(sentences_scores(sentences))[-k:]))\n",
    "\n",
    "def get_best_k_sentences(k, sentences, original_sentences):\n",
    "    return [original_sentences[idx] for idx in get_best_k_sentences_indecies(k, sentences, original_sentences)]\n",
    "\n",
    "def summarize(text, k):\n",
    "    # Apply preprocessing\n",
    "    original_sentences, sentences = apply_preprocessing(text)\n",
    "    # Summariez\n",
    "    return ' '.join(get_best_k_sentences(k, sentences, original_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main logic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the English stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "my_stopwords = stopwords.words()\n",
    "# Create the stemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File reading ..\n",
    "file_path = 'test_file.txt'\n",
    "f = open(file_path)\n",
    "text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the general problem is an object detection problem [3] which is detecting people with and without masks in offline or live videos. however the great number of images available in rmdf, it seems extremely unbalanced and needs reasonable preprocessing in order to have an unbiased model. through manual investigation for the dataset, there is no guarantee on the diversity of ethnicity of the people in its images; however, this problem may not be effective in our approach as the network would learn specific location or shape of the mask itself not the faces.\n"
     ]
    }
   ],
   "source": [
    "number_of_sentences = 3  # Number of sentence needed in the summary\n",
    "summary = summarize(text, number_of_sentences)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
